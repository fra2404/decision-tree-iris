\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{array}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{fancyhdr}

% Configure hyperref
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Iris Ensemble Learning Report},
    pdfauthor={Francesco}
}

% Configure page headers
\pagestyle{fancy}
\fancyhf{}
\rhead{Ensemble Learning on Iris Dataset}
\lhead{Francesco}
\cfoot{\thepage}

% Configure code listings
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    language=Python
}

\onehalfspacing

\title{
    \Large\textbf{Ensemble Learning Methods Applied to Iris Classification} \\
    \vspace{0.5em}
    \large Artificial Intelligence Assignment 3 \\
    \vspace{0.3em}
    \normalsize October 2025
}

\author{Francesco}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction}

\subsection{Project Overview}
This project implements and evaluates multiple ensemble learning methods for classification on the classic Iris dataset. Ensemble learning combines multiple weak learners to create a more robust and accurate model, demonstrating the principle that collective intelligence often surpasses individual performance.

\subsection{Objectives}
The primary objectives of this assignment are:
\begin{enumerate}
    \item Implement various ensemble learning algorithms (voting, bagging, stacking, random forests)
    \item Compare performance across different ensemble methods
    \item Analyze feature importance and model interpretability
    \item Optimize hyperparameters for best performance
    \item Provide comprehensive visualization and statistical analysis
\end{enumerate}

\subsection{Dataset: Iris Classification}
The Iris dataset, introduced by Ronald Fisher in 1936, remains one of the most widely used datasets in machine learning education and research. It contains:
\begin{itemize}
    \item \textbf{Samples:} 150 instances (50 per class)
    \item \textbf{Features:} 4 numerical attributes (sepal length, sepal width, petal length, petal width)
    \item \textbf{Classes:} 3 species (Iris setosa, Iris versicolor, Iris virginica)
    \item \textbf{Task:} Multi-class classification
\end{itemize}

\section{Methodology}

\subsection{System Architecture}
The implementation follows a modular architecture with four main components:

\begin{enumerate}
    \item \textbf{Data Processor} (\texttt{data\_processor.py}): Handles data loading, preprocessing, and feature engineering
    \item \textbf{Ensemble Core} (\texttt{ensemble\_core.py}): Implements all ensemble learning algorithms
    \item \textbf{Visualization} (\texttt{visualization.py}): Creates comprehensive plots and analysis reports
    \item \textbf{Main Application} (\texttt{main\_iris\_ensemble.py}): Orchestrates the complete analysis pipeline
\end{enumerate}

\subsection{Data Preprocessing}
The preprocessing pipeline includes:
\begin{itemize}
    \item \textbf{Stratified Split:} 70\% training, 30\% testing to maintain class balance
    \item \textbf{Feature Standardization:} StandardScaler normalization ($\mu=0$, $\sigma=1$)
    \item \textbf{Cross-Validation:} 5-fold stratified CV for robust evaluation
    \item \textbf{Feature Analysis:} Mutual information for importance ranking
\end{itemize}

\subsection{Ensemble Methods Implemented}

\subsubsection{Voting Classifiers}
\begin{itemize}
    \item \textbf{Hard Voting:} Majority class prediction from base classifiers
    \item \textbf{Soft Voting:} Weighted average of predicted probabilities
    \item \textbf{Base Classifiers:} Decision Tree, SVM, Logistic Regression, Naive Bayes, k-NN
\end{itemize}

\subsubsection{Bagging (Bootstrap Aggregating)}
\begin{itemize}
    \item \textbf{Algorithm:} Bootstrap sampling with aggregation
    \item \textbf{Base Estimator:} Decision Trees
    \item \textbf{Estimators:} 100 trees with out-of-bag scoring
\end{itemize}

\subsubsection{Random Forest}
\begin{itemize}
    \item \textbf{Enhancement:} Bagging + feature randomness
    \item \textbf{Configuration:} 100 trees, $\sqrt{p}$ features per split
    \item \textbf{Optimization:} Grid search for optimal parameters
\end{itemize}

\subsubsection{Stacking}
\begin{itemize}
    \item \textbf{Level 0:} Decision Tree, SVM, Naive Bayes, k-NN
    \item \textbf{Meta-learner:} Logistic Regression
    \item \textbf{Cross-validation:} 5-fold for meta-feature generation
\end{itemize}

\subsubsection{Boosting Methods}
\begin{itemize}
    \item \textbf{AdaBoost:} Adaptive boosting with decision stumps
    \item \textbf{Gradient Boosting:} Sequential error correction
    \item \textbf{Configuration:} 100 estimators, learning rate optimization
\end{itemize}

\section{Experimental Results}

\subsection{Performance Summary}
The comprehensive evaluation reveals the following performance ranking:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method} & \textbf{Test Accuracy} & \textbf{CV Mean±Std} & \textbf{F1-Score} \\
\hline
Stacking & \textbf{0.9333} & 0.9714±0.0233 & 0.9333 \\
AdaBoost & \textbf{0.9333} & 0.9333±0.0233 & 0.9333 \\
Gradient Boosting & \textbf{0.9333} & 0.9524±0.0301 & 0.9333 \\
Voting (Soft) & 0.9111 & 0.9714±0.0233 & 0.9107 \\
Voting (Hard) & 0.9111 & 0.9714±0.0233 & 0.9107 \\
Bagging & 0.9111 & 0.9524±0.0301 & 0.9107 \\
Random Forest & 0.8889 & 0.9524±0.0301 & 0.8981 \\
\hline
\end{tabular}
\caption{Ensemble Methods Performance Comparison}
\end{table}

\subsection{Key Findings}

\subsubsection{Best Performing Methods}
Three methods achieved the highest test accuracy of \textbf{93.33\%}:
\begin{enumerate}
    \item \textbf{Stacking:} Demonstrates the power of meta-learning with excellent generalization
    \item \textbf{AdaBoost:} Sequential learning effectively handles the Iris classification task
    \item \textbf{Gradient Boosting:} Strong performance through iterative error correction
\end{enumerate}

\subsubsection{Feature Importance Analysis}
Mutual information analysis reveals clear feature ranking:
\begin{enumerate}
    \item \textbf{Petal Length} (MI = 0.9926): Most discriminative feature
    \item \textbf{Petal Width} (MI = 0.9856): Strong classification power
    \item \textbf{Sepal Length} (MI = 0.5114): Moderate importance
    \item \textbf{Sepal Width} (MI = 0.2994): Least important for classification
\end{enumerate}

\subsection{Hyperparameter Optimization}
Grid search optimization improved two key methods:

\textbf{Random Forest Optimization:}
\begin{itemize}
    \item Best parameters: \{max\_depth: 5, min\_samples\_split: 2, n\_estimators: 50\}
    \item Cross-validation score: 0.9524
\end{itemize}

\textbf{Bagging Optimization:}
\begin{itemize}
    \item Best parameters: \{max\_features: 0.5, max\_samples: 0.5, n\_estimators: 200\}
    \item Cross-validation score: 0.9714 (improved from base configuration)
\end{itemize}

\subsection{Statistical Analysis}
The results demonstrate:
\begin{itemize}
    \item \textbf{Low Variance:} CV standard deviations $\leq$ 0.03 indicate stable performance
    \item \textbf{High Precision:} All methods achieve precision > 0.89
    \item \textbf{Balanced Performance:} F1-scores closely match accuracy values
    \item \textbf{Class Balance:} Perfect stratification maintained across all splits
\end{itemize}

\section{Implementation Details}

\subsection{Code Architecture}
The implementation emphasizes:
\begin{itemize}
    \item \textbf{Modularity:} Clean separation of concerns across components
    \item \textbf{Reproducibility:} Fixed random state (42) for consistent results
    \item \textbf{Extensibility:} Easy addition of new ensemble methods
    \item \textbf{Documentation:} Comprehensive docstrings and comments
\end{itemize}

\subsection{Technical Specifications}
\begin{itemize}
    \item \textbf{Language:} Python 3.8+
    \item \textbf{Core Libraries:} scikit-learn 1.7.2, pandas 2.3.3, numpy 2.3.3
    \item \textbf{Visualization:} matplotlib 3.10.7, seaborn 0.13.2
    \item \textbf{Environment:} Virtual environment with requirements.txt
\end{itemize}

\subsection{Output Generation}
The system automatically generates:
\begin{itemize}
    \item \textbf{Performance Metrics:} CSV files with detailed statistics
    \item \textbf{Visualizations:} Dataset overview, performance comparison, confusion matrices
    \item \textbf{Analysis Reports:} Comprehensive text summaries
    \item \textbf{Feature Analysis:} Importance rankings and correlation matrices
\end{itemize}

\section{Discussion}

\subsection{Ensemble Learning Effectiveness}
The results validate key ensemble learning principles:

\begin{enumerate}
    \item \textbf{Diversity Benefits:} Different algorithms capture complementary patterns
    \item \textbf{Bias-Variance Trade-off:} Ensemble methods reduce both bias and variance
    \item \textbf{Meta-learning Power:} Stacking's superior performance demonstrates effective meta-feature utilization
    \item \textbf{Sequential Learning:} Boosting methods excel through iterative improvement
\end{enumerate}

\subsection{Dataset-Specific Insights}
The Iris dataset characteristics influence ensemble performance:
\begin{itemize}
    \item \textbf{Small Size:} Limited training data favors simpler ensemble methods
    \item \textbf{Linear Separability:} Simple decision boundaries don't require complex ensembles
    \item \textbf{Feature Quality:} High mutual information scores enable good single-model performance
    \item \textbf{Class Balance:} Perfect balance simplifies the learning task
\end{itemize}

\subsection{Comparison with Previous Work}
The achieved accuracy of 93.33\% aligns with literature expectations for the Iris dataset, where:
\begin{itemize}
    \item Single algorithms typically achieve 90-95\% accuracy
    \item Ensemble methods provide incremental improvements
    \item Perfect classification is rare due to natural class overlap
\end{itemize}

\section{Conclusions}

\subsection{Key Achievements}
This project successfully demonstrates:
\begin{enumerate}
    \item \textbf{Comprehensive Implementation:} Seven distinct ensemble methods with proper evaluation
    \item \textbf{Strong Performance:} 93.33\% test accuracy with multiple top-performing methods
    \item \textbf{Robust Analysis:} Statistical validation through cross-validation and confidence intervals
    \item \textbf{Professional Code Quality:} Modular, documented, and reproducible implementation
\end{enumerate}

\subsection{Ensemble Learning Insights}
The analysis reveals several important principles:
\begin{itemize}
    \item \textbf{Method Selection:} Stacking and boosting excel for complex pattern recognition
    \item \textbf{Feature Importance:} Petal measurements dominate Iris classification
    \item \textbf{Hyperparameter Impact:} Optimization provides meaningful performance gains
    \item \textbf{Evaluation Robustness:} Cross-validation essential for reliable assessment
\end{itemize}

\subsection{Practical Applications}
The implemented framework can be extended to:
\begin{itemize}
    \item \textbf{Larger Datasets:} Scale to real-world classification problems
    \item \textbf{Additional Methods:} Incorporate deep learning and modern ensemble techniques
    \item \textbf{Feature Engineering:} Apply advanced preprocessing and selection methods
    \item \textbf{Production Deployment:} Adapt for real-time prediction systems
\end{itemize}

\subsection{Future Work}
Potential improvements include:
\begin{enumerate}
    \item \textbf{Advanced Ensembles:} Explore XGBoost, LightGBM, and neural network ensembles
    \item \textbf{Automated ML:} Implement automated hyperparameter optimization
    \item \textbf{Interpretability:} Add SHAP analysis and model explanation capabilities
    \item \textbf{Comparative Studies:} Evaluate on additional classification benchmarks
\end{enumerate}

This project demonstrates the power of ensemble learning while providing a solid foundation for advanced machine learning applications.

\end{document}