
\documentclass[conference]{IEEEtran}
% Fix for UTF-8 encoding issues
\usepackage[utf8]{inputenc}
% Additional packages
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{hyperref}
\usepackage{subcaption}
\definecolor{headergray}{gray}{0.6}
\addtolength{\topmargin}{10pt}

% Title and author block
\title{Iris Classification with Ensemble Learning: Modular Implementation and Analysis}
\author{
\IEEEauthorblockN{Francesco Albano}
\IEEEauthorblockA{
Student ID: LJ2506219\\
School of Computer Science\\
Beihang University (BUAA)\\
Beijing, China\\
Email: francescoalbano@buaa.edu.cn, francescoalbano357@gmail.com}
}
\begin{document}

\pagestyle{fancy}
\fancyhead[C]{\textcolor{headergray}{Assignment 3 – Student ID: LJ2506219 – Artificial Intelligence}}
\renewcommand{\headrulewidth}{0pt}
\fancyfoot[C]{\thepage}
\setlength{\footskip}{40pt}

\maketitle

\begin{abstract}
This report presents a modular implementation of ensemble learning methods for multi-class classification on the Iris dataset. The system achieves up to 93.33\% test accuracy. Bias-variance trade-off, feature importance, and method comparison are discussed, with results and code structure provided for reproducibility.
\end{abstract}

\begin{IEEEkeywords}
ensemble learning, iris dataset, classification, machine learning, data preprocessing, interpretability
\end{IEEEkeywords}


\section{Introduction}
This project implements an Iris species classification system using multiple ensemble learning algorithms, following a modular and reproducible approach. The goal is to combine theoretical rigor with practical data science skills, focusing on modularity, accuracy, and interpretability. Ensemble methods are chosen for their ability to improve predictive performance and robustness by combining multiple base learners. The modular approach ensures that each phase of the workflow (preprocessing, learning, visualization) can be understood, tested, and improved separately.

\section{System Architecture}
The project is organized in four modules: (1) \textbf{data\_processor.py} for data loading and preprocessing, (2) \textbf{ensemble\_core.py} for ensemble algorithms and evaluation, (3) \textbf{visualization.py} for plots and exports, (4) \textbf{main\_iris\_ensemble.py} for pipeline orchestration. This modularity ensures clarity, reusability, and easy testing.

\section{Methodology}
\noindent
	\textbf{Dataset:} Iris (150 samples, 4 features, 3 balanced classes).\\
	\textbf{Preprocessing:} Stratified 70/30 split, standardization ($\mu=0$, $\sigma=1$), 5-fold cross-validation, mutual information for feature ranking.\\
	\textbf{Ensemble Methods:} Soft voting (Decision Tree, SVM, Logistic Regression, Naive Bayes, k-NN), Bagging (Decision Trees), Random Forest (grid search), Stacking (meta-learner: Logistic Regression).\\
All code is modular and reproducible.




\section{Results and Analysis}


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{../results/iris_dataset_overview.png}
\caption{Overview of the main statistical and visual analyses performed on the Iris dataset. (Top-left) Sepal length vs sepal width scatter plot highlights the relationship between two key features. (Top-center) Feature distribution histograms show the statistical properties of each measurement. (Top-right) Feature correlation matrix quantifies linear relationships among features. (Bottom-left) Feature distribution by species reveals class-specific patterns. (Bottom-center) PCA visualization illustrates class separability in reduced dimensions. (Bottom-right) Class distribution confirms perfect balance across all three species.}
\label{fig:dataset_overview}
\end{figure*}

\textbf{Key Observations from Figure \ref{fig:dataset_overview}:}
\begin{itemize}
    \item \textbf{Class Separability:} Iris setosa is linearly separable from the other classes
    \item \textbf{Feature Correlation:} Strong correlation (0.96) between petal length and width
    \item \textbf{Distribution Patterns:} Some features show bimodal distributions due to species differences
    \item \textbf{Perfect Balance:} Equal representation (50 samples) for each class
\end{itemize}


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{../results/ensemble_performance_comparison.png}
\caption{Ensemble Methods Performance Analysis. (Top-left) Training vs. testing accuracy for each method, highlighting possible overfitting or generalization gaps. (Top-right) Cross-validation performance distribution, showing the stability and variance of each method across folds. (Bottom-left) Multi-metric performance comparison (accuracy, precision, recall, F1-score) for a holistic evaluation. (Bottom-right) Overall performance ranking, summarizing the relative effectiveness of all ensemble methods.}
\label{fig:performance_comparison}
\end{figure*}

\textbf{Performance Analysis from Figure \ref{fig:performance_comparison}:}
\begin{itemize}
    \item \textbf{Top Performers:} Stacking, AdaBoost, and Gradient Boosting achieve 93.33\% test accuracy
    \item \textbf{Consistency:} Low variance across CV folds indicates stable performance
    \item \textbf{Overfitting:} Bagging shows perfect training accuracy but lower test performance
    \item \textbf{Ensemble Benefit:} All ensemble methods outperform typical single classifier baselines
\end{itemize}


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{../results/feature_importance_analysis.png}
\caption{Feature Importance Analysis. (Left) Feature importance heatmap summarizing the discriminative power of each feature. (Right) Feature importance by method, comparing the rankings provided by different ensemble algorithms. Both views consistently identify petal measurements as the most informative features.}
\label{fig:feature_importance}
\end{figure*}

\textbf{Feature Insights from Figure \ref{fig:feature_importance}:}
\begin{itemize}
    \item \textbf{Petal Dominance:} Petal length (MI=0.99) and width (MI=0.99) are highly discriminative
    \item \textbf{Sepal Contribution:} Sepal measurements provide moderate additional information
    \item \textbf{Method Consistency:} Both mutual information and Random Forest rankings agree
    \item \textbf{Biological Relevance:} Petal characteristics are indeed key species identifiers
\end{itemize}


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{../results/confusion_matrices.png}
\caption{Confusion Matrices for Top Performing Methods: Normalized confusion matrices showing prediction accuracy for each class. Perfect diagonal values indicate correct classifications, while off-diagonal values reveal inter-class confusion patterns. All top methods achieve near-perfect classification with minimal confusion between versicolor and virginica.}
\label{fig:confusion_matrices}
\end{figure*}

\textbf{Classification Pattern Analysis from Figure \ref{fig:confusion_matrices}:}
\begin{itemize}
    \item \textbf{Perfect Setosa Classification:} All methods achieve 100\% accuracy for setosa
    \item \textbf{Versicolor-Virginica Confusion:} Minor confusion between these morphologically similar species
    \item \textbf{Method Differences:} Stacking shows slightly better discrimination between similar classes
    \item \textbf{Biological Consistency:} Classification errors align with known taxonomic similarities
\end{itemize}

\subsection{Quantitative Performance Summary}

The comprehensive evaluation reveals the following performance ranking, summarized in Table~\ref{tab:ensemble_performance}:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method} & \textbf{Test Accuracy} & \textbf{CV Mean±Std} & \textbf{F1-Score} \\
\hline
Stacking & \textbf{0.9333} & 0.9714±0.0233 & 0.9333 \\
Voting (Soft) & 0.9111 & 0.9714±0.0233 & 0.9107 \\
Bagging & 0.9111 & 0.9524±0.0301 & 0.9107 \\
Random Forest & 0.8889 & 0.9524±0.0301 & 0.8878 \\
\hline
\end{tabular}
\caption{Essential Ensemble Methods Performance Comparison}
\label{tab:ensemble_performance}
\end{table}

\subsection{Key Findings}

\subsubsection{Best Performing Method}
\textbf{Stacking} achieved the highest test accuracy of \textbf{93.33\%}, demonstrating the effectiveness of meta-learning approaches that combine predictions from multiple base classifiers through a meta-learner.

\subsubsection{Feature Importance Analysis}
Mutual information analysis reveals clear feature ranking:
\begin{enumerate}
    \item \textbf{Petal Length} (MI = 0.9926): Most discriminative feature
    \item \textbf{Petal Width} (MI = 0.9856): Strong classification power
    \item \textbf{Sepal Length} (MI = 0.5114): Moderate importance
    \item \textbf{Sepal Width} (MI = 0.2994): Least important for classification
\end{enumerate}

\subsection{Hyperparameter Optimization}
Grid search optimization improved two key methods:

\textbf{Random Forest Optimization:}
\begin{itemize}
    \item Best parameters: \{max\_depth: 5, min\_samples\_split: 2, n\_estimators: 50\}
    \item Cross-validation score: 0.9524
\end{itemize}

\textbf{Bagging Optimization:}
\begin{itemize}
    \item Best parameters: \{max\_features: 0.5, max\_samples: 0.5, n\_estimators: 200\}
    \item Cross-validation score: 0.9714 (improved from base configuration)
\end{itemize}

\subsection{Statistical Analysis}
The results demonstrate:
\begin{itemize}
    \item \textbf{Low Variance:} CV standard deviations $\leq$ 0.03 indicate stable performance
    \item \textbf{High Precision:} All methods achieve precision $\geq$ 0.89
    \item \textbf{Balanced Performance:} F1-scores closely match accuracy values
    \item \textbf{Class Balance:} Perfect stratification maintained across all splits
\end{itemize}

\section{Implementation Details}

\subsection{Code Architecture}
The implementation emphasizes:
\begin{itemize}
    \item \textbf{Modularity:} Clean separation of concerns across components
    \item \textbf{Reproducibility:} Fixed random state (42) for consistent results
    \item \textbf{Extensibility:} Easy addition of new ensemble methods
    \item \textbf{Documentation:} Comprehensive docstrings and comments
\end{itemize}

\subsection{Technical Specifications}
\begin{itemize}
    \item \textbf{Language:} Python 3.8+
    \item \textbf{Core Libraries:} scikit-learn 1.7.2, pandas 2.3.3, numpy 2.3.3
    \item \textbf{Visualization:} matplotlib 3.10.7, seaborn 0.13.2
    \item \textbf{Environment:} Virtual environment with requirements.txt
\end{itemize}

\subsection{Output Generation}
The system automatically generates:
\begin{itemize}
    \item \textbf{Performance Metrics:} CSV files with detailed statistics
    \item \textbf{Visualizations:} Dataset overview, performance comparison, confusion matrices
    \item \textbf{Analysis Reports:} Comprehensive text summaries
    \item \textbf{Feature Analysis:} Importance rankings and correlation matrices
\end{itemize}

\section{Discussion}

\subsection{Ensemble Learning Effectiveness}
The results validate key ensemble learning principles:

\begin{enumerate}
    \item \textbf{Diversity Benefits:} Different algorithms capture complementary patterns
    \item \textbf{Bias-Variance Trade-off:} Ensemble methods reduce both bias and variance
    \item \textbf{Meta-learning Power:} Stacking's superior performance demonstrates effective meta-feature utilization
    \item \textbf{Sequential Learning:} Boosting methods excel through iterative improvement
\end{enumerate}

\subsection{Dataset-Specific Insights}
The Iris dataset characteristics influence ensemble performance:
\begin{itemize}
    \item \textbf{Small Size:} Limited training data favors simpler ensemble methods
    \item \textbf{Linear Separability:} Simple decision boundaries don't require complex ensembles
    \item \textbf{Feature Quality:} High mutual information scores enable good single-model performance
    \item \textbf{Class Balance:} Perfect balance simplifies the learning task
\end{itemize}

\subsection{Comparison with Previous Work}
The achieved accuracy of 93.33\% aligns with literature expectations for the Iris dataset, where:
\begin{itemize}
    \item Single algorithms typically achieve 90-95\% accuracy
    \item Ensemble methods provide incremental improvements
    \item Perfect classification is rare due to natural class overlap
\end{itemize}


\section{Conclusions}
This work demonstrates that ensemble learning, when applied with a modular and reproducible approach, can achieve excellent results even on classic datasets like Iris. The pipeline is clear, extensible, and well documented. The results confirm that model diversity and proper validation lead to robust and interpretable performance. The code and structure are designed to be easily reused and adapted to new problems.

\section*{Acknowledgment}
This report was prepared as Assignment 3 for the course ``Artificial Intelligence'' at Beihang University (BUAA).

\end{document}